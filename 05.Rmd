---
title: "Untitled"
output:
  word_document: default
  pdf_document: default
  html_document: default
---
# 1.

## a.

### (i)
7/10 = 0.7

### (ii)
3/10 = 0.3

### (iii)
```{r}
(4/7)*(4/7)*(2/7)
```


### (iv)
```{r}
1/3.0*2/3.0*1/3.0
```



## 1b.
Suppose A = 'Popularity = P' , B = 'Price = 'dollar',Delivery = ‘Yes’ , Cuisine = ‘Korean’'
$$Pr(A|B) = \frac{Pr(A,B)}{Pr(B)} 
= \frac{Pr(B|A)*Pr(A)}{Pr(B|A)*Pr(A) + Pr(B|A^c)*Pr(A^c)}$$

```{r}
((4/7)*(4/7)*(2/7)*0.7)/((4/7)*(4/7)*(2/7)*0.7 + 1/3.0*2/3.0*1/3.0*0.3)
```

Yes. Since the probability as popular is higher than Not popular.

## 1c.
Rerun the process multiple times. Each time, randomly select a certain number pieces of data with replacement from the original data, and calculate the probability of popular during each iteration. Return the average of the probabilities as the final probability, and classified it as popular or not.

## 1d.

Sensitivity measures the ratio of true positive in positive cases, which can handle the rare positive problem

# 2.

## a.

```{r}
data = data.frame(
  x1 = c(1, 2, 2.5, 3, 1.5, 2.3, 1.2, 0.8),
  x2 = c(0.5, 1.2, 2, 2, 2, 3, 1.9, 1),
  y = factor(c(1, 1, 1, 1, -1, -1, -1, -1))
  )
  test = data.frame(
  x1 = c(2.7, 2.5, 1.5, 1.2),
  x2 = c(2.7, 1, 2.5, 1),
  y = factor(c(1, 1, -1, -1))
  )
```
```{r message = FALSE}
library(FNN)
library(caret)
```

```{r warning = FALSE, message=FALSE}
knn_1 = train(y ~ ., data = data, method = "knn", tuneGrid = expand.grid(k = 1))
```

```{r}
pred = predict(knn_1, newdata = test)
pred
```
```{r}
pred != test$y
```


The prediction of KNN with K = 1 is (-1,1,-1,-1)
Only one of them is inaccurate. Testing error is 1

## b.
```{r warning = FALSE, message = FALSE}
knn_3 = train(y ~ ., data = data, method = "knn", tuneGrid = expand.grid(k = 3))
```



```{r}
pred = predict(knn_3, newdata = test)
pred
```

```{r}
pred != test$y
```

The prediction of KNN with K = 3 is (-1,1,-1,-1)
Only one of them is inaccurate. Testing error is 1

## c.
```{r}
data = data.frame(x1 = c(1,2,2.5,3,1.5,2.3,1.2,0.8),
                  x2 = c(0.5,1.2,2,2,2,3,1.9,1),
                  y = c(1,1,1,1,-1,-1,-1,-1)
                  )
test = data.frame(x1 = c(2.7,2.5,1.5,1.2),
                  x2 = c(2.7,1,2.5,1),
                  y = c(1,1,-1,-1)
                  )
```


```{r}
l = lm(y~., data = data)
```
```{r}
l
```
```{r}
cat("a = ", l$coefficients[2])
cat(" b = ", l$coefficients[3])
cat(" c = ", l$coefficients[1])
```


```{r}
pred = predict(l, newdata = test)
```


```{r}
result = c()
for (i in pred){
  if (i > 0){
    result = c(result,'+1')
  }
  else{
      result =c(result,'-1')
  }
}
```
```{r}
result 
```

the predicitions of testing set are (-1,1,-1,1)
the testing error is 2

```{r}
pred = predict(l, newdata = data)
```


```{r}
result = c()
for (i in pred){
  if (i > 0){
    result = c(result,'+1')
  }
  else{
      result =c(result,'-1')
  }
}
```
```{r}
result 
```


The predictions of training set are (+1,+1,+1,+1,-1,-1,-1,-1)
the trainning error is 0


## d.
From 2a - 2c, KNN has a higher accuracy. Accuracy for KNN method is higher than linear classification method. KNN method performs better than linear classification method.


# 3.

## a.
```{r message = FALSE}
library(NbClust)
```


```{r}
index = c(1,2,3,4,5,6,7,8,9,10,11,12,13)
d = data.frame(x1 = c(1,1,2,2,2,3,5,4,4,5,5,6,6),
               x2 = c(3,2,1,2,3,2,3,3,5,4,5,4,5))
```


```{r}
c1 = c(0,6)
c2 = c(4,5)
km_out = kmeans(d,rbind(c1,c2))
```

```{r}
plot(d, col=km_out$cluster)
```

```{r}
index[km_out$cluster == 1]
index[km_out$cluster == 2]
```


cluster 1 has index from 1 to 6, cluster 2 has index from 7 to 13

## b

```{r}
library(dbscan)
```
```{r warning = FALSE}
par(mfrow=c(3,3))
dbscluster = dbscan(d,eps = 1.5, MinPts = 2, showplot = 1)
```


```{r}
hullplot(d, dbscluster, main = "DBSCAN")
```



```{r}
dbscluster$cluster
```

All values belong to same cluster



## 3c
```{r}
library(cluster)
```
```{r}
age_clust = agnes(d,metric= 'Euclidean', method = 'single')
```

```{r}
age_clust
```

```{r}
par(mfrow=c(1,2))
plot(age_clust)
```




